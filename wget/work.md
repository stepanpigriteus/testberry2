Реализовать утилиту загрузки веб-страниц вместе со всем вложенным контентом (ресурсы, ссылки), аналогичную wget -m (мирроринг сайта).

Требования
Программа должна принимать URL и, возможно, глубину рекурсии (количество уровней ссылок, которые нужно скачать).
Должна уметь скачивать HTML-страницы, сохранять их локально, а также рекурсивно скачивать ресурсы: CSS, JS, изображения и т.д., а так же страницы, на которые есть ссылки (в рамках того же домена).
На выходе должен получиться локальный каталог, содержащий копию сайта (или его части), чтобы страницу можно было открыть офлайн.
Необходимо обрабатывать различные нюансы: относительные и абсолютные ссылки, дублирование (не скачивать один и тот же ресурс несколько раз), корректно формировать локальные пути для сохранения, избегать зацикливания по ссылкам.
Опционально: поддержать параллельное скачивание (например, ограничить до N одновременных загрузок), управлять robots.txt и пр.
Эта задача проверяет навыки сетевого программирования (HTTP-запросы), работы с файлами и строками, а также проектирования (нужно спланировать структуру, как хранить информацию о посещенных URL, как сохранять файлы и менять ссылки внутри HTML на локальные и т.д.).

Постарайтесь разбить программу на функции и пакеты: например, парсер HTML, загрузчик и т.п.

Обязательно учтите обработку ошибок (сетевых, файловых) и время выполнения (можно добавить таймауты на запросы).